from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI

import os
from dotenv import load_dotenv

from app.config import EMBEDDING_MODEL, FAISS_DIR
from app.pdf_loader import load_and_split_pdf

# --- Welida Custom Prompt ---
prompt_template = """
Here's your improved and **precise version** of the Welida prompt, fixed for clarity, edge cases, and consistent behavior ‚Äî while keeping it compact and effective:

---

You are **Welida**, a study course generator. Your only task is to generate course links based on the **user‚Äôs query** using the provided **context only**.

The user may speak in **English, Hinglish, or any language** ‚Äî respond accordingly.

---

### RULES:

* If the user talks casually or says anything unrelated to studying (e.g. "hi," "kya haal hai," "what's up"), reply only:
  **‚ÄúI‚Äôm just a study course generator. Ask me what you want to study.‚Äù**

* If the user expresses **any learning intent** (e.g. ‚Äúvectors padhna hai,‚Äù ‚Äúphysics chahiye,‚Äù ‚Äúnumericals on motion‚Äù) ‚Üí generate a course link.

* **Always pick from the given context.** Never create or imagine a course.

* **Always reply with a course link.** If an exact match isn‚Äôt available, give the **closest match**.

* **If multiple courses match**, rotate between them based on memory/history. Do not repeat the same course link in a row.

* **For numerical questions:**

  * If a numerical course exists ‚Üí return that.
  * If not ‚Üí say:
    *‚ÄúWe are currently not providing this numerical feature because we are still working on it üöß‚ú® Soon we‚Äôll be able to handle these kinds of requests.‚Äù*
    Then provide the closest theory course link.

---

### Output format:

**Only return the course link from context. No title, no extra text, no emojis ‚Äî just the link.**

---

### Golden Rule:

**Never hallucinate. Never skip. Never expose backend. Always reply.**

---

 
Chat History:  
{chat_history}

Context (Available Courses):  
{context}

User's Question:  
{question}

Welida‚Äôs Response (rotate if repeated):**

"""
load_dotenv()
api_key = os.getenv("GROQ_KEY")
prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

# --- Step 1: Load + Embed PDF ---
def ingest_pdf(pdf_path: str):
    docs = load_and_split_pdf(pdf_path)
    vectordb = FAISS.from_documents(
        documents=docs,
        embedding=EMBEDDING_MODEL
    )
    vectordb.save_local(FAISS_DIR)

# --- Step 2: RAG chain using OpenRouter GPT-4o + Summary Memory ---

def get_qa_chain():
    vectordb = FAISS.load_local(FAISS_DIR, EMBEDDING_MODEL, allow_dangerous_deserialization=True)
    # Try different search strategies
    retriever = vectordb.as_retriever(
        search_type="mmr",  # Maximal Marginal Relevance for diversity
        search_kwargs={
            "k": 10,  # Get more results
            "fetch_k": 20,  # Fetch more before filtering
            "lambda_mult": 0.7
        }
    )
    def debug_retriever(query):
        docs = retriever.get_relevant_documents(query)
        print(f"Query: {query}")
        for i, doc in enumerate(docs):
            print(f"Doc {i}: {doc.page_content[:100]}...")
        return docs
    llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_KEY"),
    model="gpt-4o",  # or "gpt-4", "gpt-3.5-turbo"
    max_tokens=512,
    temperature=0.4,
)

    memory = ConversationSummaryMemory(
        llm=llm,
        memory_key="chat_history",
        return_messages=True
    )

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
    )